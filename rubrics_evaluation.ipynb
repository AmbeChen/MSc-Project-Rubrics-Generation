{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28531e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, json, tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b03ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d7e52",
   "metadata": {},
   "source": [
    "Exact Reference rubrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ade4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# ‚úÖ Load filtered dataset from saved file\n",
    "with open(\"outputs/filtered/rubrics_8_15.jsonl\") as f:\n",
    "    filtered_data = [json.loads(line) for line in f]\n",
    "\n",
    "# ‚úÖ Convert to HuggingFace Dataset object\n",
    "filtered_dataset = Dataset.from_list(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Load the embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ac938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Load reference rubrics\n",
    "def extract_ref_rubrics(dataset, output_dir=\"outputs/ref_rubrics\", max_items=None):\n",
    "    \"\"\"\n",
    "    Extract reference rubrics from filtered_dataset and store them as the ref_rubrics_{i}.json file by index.\n",
    "\n",
    "    Args:\n",
    "    - dataset: filtered HuggingFace dataset from which to extract referecne rubrics\n",
    "    - output_dir: save path\n",
    "    - max_items: Limit the number of saves (optional)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    saved = 0\n",
    "    for i, example in tqdm(enumerate(dataset), desc=\"Saving reference rubrics\"):\n",
    "        if max_items and saved >= max_items:\n",
    "            break\n",
    "\n",
    "        rubrics = example.get(\"rubrics\", None)\n",
    "        if rubrics:\n",
    "            out_path = os.path.join(output_dir, f\"ref_rubrics_{i}.json\")\n",
    "            with open(out_path, \"w\") as f:\n",
    "                json.dump(rubrics, f, indent=2, ensure_ascii=False)\n",
    "            saved += 1\n",
    "\n",
    "    print(f\"‚úÖ Saved {saved} reference rubrics to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_ref_rubrics(filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2a03b",
   "metadata": {},
   "source": [
    "Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88026a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ All possible axes\n",
    "ALL_AXES = [\n",
    "    \"completeness\",\n",
    "    \"accuracy\",\n",
    "    \"context_awareness\",\n",
    "    \"communication_quality\",\n",
    "    \"instruction_following\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rubrics(conversation_id, threshold=0.5, allow_axis_mismatch=True):\n",
    "    conversation_path = f\"outputs/prompts/conversation_{conversation_id}.txt\"\n",
    "    generated_path = f\"outputs/rubrics/rubrics_{conversation_id}.json\"\n",
    "    reference_path = f\"outputs/ref_rubrics/ref_rubrics_{conversation_id}.json\"\n",
    "    os.makedirs(\"outputs/eval\", exist_ok=True)\n",
    "    os.makedirs(\"outputs/eval/matched_pair\", exist_ok=True)\n",
    "    os.makedirs(\"outputs/eval/eval\", exist_ok=True)\n",
    "\n",
    "    with open(conversation_path) as f:\n",
    "        conversation = f.read().strip()\n",
    "\n",
    "    with open(reference_path) as f:\n",
    "        references = json.load(f)\n",
    "\n",
    "    with open(generated_path) as f:\n",
    "        generated = json.load(f)\n",
    "\n",
    "    ref_axes = set()\n",
    "    for r in references:\n",
    "        for tag in r.get(\"tags\", []):\n",
    "            if tag.startswith(\"axis:\"):\n",
    "                ref_axes.add(tag.split(\":\")[-1])\n",
    "    print(f\"\\nüìä Evaluation for Conversation #{conversation_id}\")\n",
    "    print(f\"üîç Conversation Content:\\n{conversation[:300]}...\")\n",
    "    print(f\"Similarity Threshold: {threshold}\")\n",
    "    print(f\"Generated Rubrics: {len(generated)}\")\n",
    "    print(f\"Reference Rubrics: {len(references)}\")\n",
    "    print(f\"Axes in Reference: {sorted(ref_axes)}\")\n",
    "\n",
    "    match_results = []\n",
    "    axis_stats = {axis: {\"matched\": 0, \"score_sum\": 0.0, \"conversation_count\": 0} for axis in ALL_AXES}\n",
    "    axis_present = {axis: False for axis in ALL_AXES}\n",
    "\n",
    "    for ref in references:\n",
    "        ref_criterion = ref[\"criterion\"]\n",
    "        ref_point = ref.get(\"points\", 0)\n",
    "        ref_axis = next((tag.split(\":\")[-1] for tag in ref.get(\"tags\", []) if tag.startswith(\"axis:\")), None)\n",
    "\n",
    "        if ref_axis:\n",
    "            axis_present[ref_axis] = True\n",
    "\n",
    "        candidates = generated if allow_axis_mismatch else [g for g in generated if g.get(\"axis\") == ref_axis]\n",
    "        if not candidates:\n",
    "            continue\n",
    "\n",
    "        ref_emb = embedder.encode([ref_criterion])\n",
    "        gen_embs = embedder.encode([g[\"criterion\"] for g in candidates])\n",
    "        sims = cosine_similarity(ref_emb, gen_embs)[0]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        best_sim = float(sims[best_idx])\n",
    "        best_gen = candidates[best_idx]\n",
    "        gen_point = best_gen.get(\"point\", 0)\n",
    "        point_diff = abs(ref_point - gen_point)\n",
    "\n",
    "        # weight = 1.0 if point_diff <= 2 else 0.5 if point_diff <= 5 else 0.0\n",
    "        # ‚úÖ New weighting logic\n",
    "        if (ref_point >= 0 and gen_point >= 0) or (ref_point < 0 and gen_point < 0):\n",
    "            weight = 0.5  # If the positive or negative scores are the same, give the base score first\n",
    "        else:\n",
    "            weight = 0.0  # A score of 0 will be given directly if the positive or negative values are inconsistent\n",
    "\n",
    "        if point_diff <= 3:\n",
    "            weight += 0.5  # Precise consistency\n",
    "        elif point_diff <= 5:\n",
    "            weight += 0.3  # Roughly close\n",
    "        # If the score exceeds 5 points, no additional weight will be added\n",
    "\n",
    "        match_score = best_sim * weight\n",
    "\n",
    "        is_similar = best_sim >= threshold\n",
    "        is_axis_match = (best_gen.get(\"axis\") == ref_axis)\n",
    "\n",
    "        if ref_axis:\n",
    "            axis_stats[ref_axis][\"score_sum\"] += match_score\n",
    "            axis_stats[ref_axis][\"matched\"] += 1\n",
    "            axis_stats[ref_axis][\"avg_score\"] = (\n",
    "                axis_stats[ref_axis][\"score_sum\"] / axis_stats[ref_axis][\"matched\"]\n",
    "                if axis_stats[ref_axis][\"matched\"] > 0 else None\n",
    "            )\n",
    "\n",
    "        match_results.append({\n",
    "            \"ref_criterion\": ref_criterion,\n",
    "            \"ref_axis\": ref_axis,\n",
    "            \"ref_point\": ref_point,\n",
    "            \"gen_criterion\": best_gen[\"criterion\"],\n",
    "            \"gen_axis\": best_gen.get(\"axis\"),\n",
    "            \"gen_point\": gen_point,\n",
    "            \"similarity\": round(best_sim, 4),\n",
    "            \"point_diff\": point_diff,\n",
    "            \"score\": round(match_score, 3),\n",
    "            \"similar_match\": is_similar,\n",
    "            \"axis_match\": is_axis_match\n",
    "        })\n",
    "\n",
    "    for axis in ref_axes:\n",
    "        axis_stats[axis][\"conversation_count\"] += 1\n",
    "\n",
    "    matched_count = len(match_results)\n",
    "    total_ref = len(references)\n",
    "    avg_score = np.mean([r[\"score\"] for r in match_results]) if match_results else 0.0\n",
    "    avg_point_diff = np.mean([r[\"point_diff\"] for r in match_results]) if match_results else 0.0\n",
    "\n",
    "    print(f\"\\n‚úÖ Matched: {matched_count} / {total_ref}\")\n",
    "    print(f\"Average Matching Score: {avg_score:.3f}\")\n",
    "    print(f\"Average Point Difference: {avg_point_diff:.2f}\")\n",
    "\n",
    "    print(\"\\nüìä Axis-level Breakdown:\")\n",
    "    for axis in ALL_AXES:\n",
    "        stat = axis_stats[axis]\n",
    "        if stat[\"conversation_count\"] == 0:\n",
    "            print(f\"- {axis:25s} | ‚ùå Missing in this conversation\")\n",
    "        else:\n",
    "            avg = stat[\"score_sum\"] / stat[\"matched\"] if stat[\"matched\"] > 0 else 0.0\n",
    "            print(f\"- {axis:25s} | Matched: {stat['matched']:2d} | Avg Score: {avg:.3f}\")\n",
    "\n",
    "    # Save\n",
    "    with open(f\"outputs/eval/matched_pair/matched_pairs_{conversation_id}.json\", \"w\") as f:\n",
    "        json.dump(match_results, f, indent=2)\n",
    "\n",
    "    summary = {\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"matched\": matched_count,\n",
    "        \"total_reference\": total_ref,\n",
    "        \"avg_score\": avg_score,\n",
    "        \"avg_point_diff\": avg_point_diff,\n",
    "        \"axis_stats\": axis_stats\n",
    "    }\n",
    "    with open(f\"outputs/eval/eval/eval_{conversation_id}.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "evaluate_rubrics(conversation_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2036d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_evaluate_rubrics(total_count, verbose=False):\n",
    "    os.makedirs(\"outputs/eval/summary\", exist_ok=True)\n",
    "    \n",
    "    all_scores = []\n",
    "    skipped = 0\n",
    "    \n",
    "    axis_global = {\n",
    "        axis: {\"score_sum\": 0.0, \"matched\": 0, \"count\": 0} for axis in ALL_AXES\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(total_count), desc=\"Evaluating rubrics\"):\n",
    "        gen_path = f\"outputs/rubrics/rubrics_{i}.json\"\n",
    "        ref_path = f\"outputs/ref_rubrics/ref_rubrics_{i}.json\"\n",
    "\n",
    "        # üí° 1. Skip if generated rubrics or reference rubrics don't exist\n",
    "        if not (os.path.exists(gen_path) and os.path.exists(ref_path)):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # üí° 2. Skip if generated rubrics is empty or only contains empty list / dict\n",
    "        with open(gen_path) as f:\n",
    "            gen_rubrics = json.load(f)\n",
    "        if not gen_rubrics or (isinstance(gen_rubrics, list) and len(gen_rubrics) == 0):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            summary = evaluate_rubrics(i)\n",
    "            all_scores.append(summary)\n",
    "            \n",
    "            axis_stats = summary.get(\"axis_stats\", {})\n",
    "            for axis in ALL_AXES:\n",
    "                axis_info = axis_stats.get(axis, {})\n",
    "                score_sum = axis_info.get(\"score_sum\", 0.0)\n",
    "                matched = axis_info.get(\"matched\", 0)\n",
    "                count = axis_info.get(\"conversation_count\", 0)\n",
    "\n",
    "                axis_global[axis][\"score_sum\"] += score_sum\n",
    "                axis_global[axis][\"matched\"] += matched\n",
    "                axis_global[axis][\"count\"] += count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Evaluation failed for conversation {i}: {e}\")\n",
    "            skipped += 1\n",
    "\n",
    "    # ‚úÖ Compute overall metrics\n",
    "    total_evals = len(all_scores)\n",
    "    if total_evals == 0:\n",
    "        print(\"‚ö†Ô∏è No successful evaluations.\")\n",
    "        return\n",
    "    \n",
    "    axis_global_avg = {\n",
    "        axis: round(axis_global[axis][\"score_sum\"] / axis_global[axis][\"matched\"], 4)\n",
    "        if axis_global[axis][\"matched\"] > 0 else None\n",
    "        for axis in ALL_AXES\n",
    "    }\n",
    "\n",
    "    avg_score = np.mean([s[\"avg_score\"] for s in all_scores])\n",
    "    avg_point_diff = np.mean([s[\"avg_point_diff\"] for s in all_scores])\n",
    "    total_matched = sum([s[\"matched\"] for s in all_scores])\n",
    "    total_ref = sum([s[\"total_reference\"] for s in all_scores])\n",
    "\n",
    "    print(f\"\\nüéØ Evaluation completed.\")\n",
    "    print(f\"‚úÖ Successfully evaluated: {total_evals}\")\n",
    "    print(f\"‚õî Skipped: {skipped}\")\n",
    "    print(f\"üìä Overall avg match score: {avg_score:.3f}\")\n",
    "    print(f\"üìâ Overall avg point difference: {avg_point_diff:.2f}\")\n",
    "    print(f\"üî¢ Total matched / total reference: {total_matched} / {total_ref} ({total_matched/total_ref:.2%})\")\n",
    "\n",
    "    # save summary\n",
    "    with open(\"outputs/eval/summary/batch_eval_summary.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"evaluated\": total_evals,\n",
    "            \"skipped\": skipped,\n",
    "            \"overall_avg_score\": avg_score,\n",
    "            \"overall_avg_point_diff\": avg_point_diff,\n",
    "            \"global_axis_scores\": axis_global_avg,\n",
    "            \"per_axis_stats\": axis_global,\n",
    "            \"total_matched\": total_matched,\n",
    "            \"total_reference\": total_ref,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd210e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_evaluate_rubrics(total_count=2735)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
