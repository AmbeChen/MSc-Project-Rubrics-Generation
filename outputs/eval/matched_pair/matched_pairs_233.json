[
  {
    "ref_criterion": "Advises saying \"I'd love to help, but I need a bit more context.\"",
    "ref_axis": "context_awareness",
    "ref_point": 10,
    "gen_criterion": "Correctly advises the user to consult a healthcare professional for personalized advice.",
    "gen_axis": "context_awareness",
    "gen_point": 8,
    "similarity": 0.4304,
    "point_diff": 2,
    "score": 0.43,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "Enquires whether the user is looking for a morning routine, a daily inspirational message, a schedule breakdown, or something else entirely.",
    "ref_axis": "context_awareness",
    "ref_point": 8,
    "gen_criterion": "Asks the user to provide more context about their daily morning routine to better understand their query.",
    "gen_axis": "context_awareness",
    "gen_point": 4,
    "similarity": 0.6569,
    "point_diff": 4,
    "score": 0.525,
    "similar_match": true,
    "axis_match": true
  },
  {
    "ref_criterion": "Offers to assist once that context is provided.",
    "ref_axis": "context_awareness",
    "ref_point": 8,
    "gen_criterion": "Provides a correct and natural language response.",
    "gen_axis": "communication_quality",
    "gen_point": 8,
    "similarity": 0.4503,
    "point_diff": 0,
    "score": 0.45,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Includes a motivational quote without any strong suggestion that such a response was requested or necessary.",
    "ref_axis": "communication_quality",
    "ref_point": -6,
    "gen_criterion": "Provides a response that is not relevant to the user's query.",
    "gen_axis": "accuracy",
    "gen_point": -8,
    "similarity": 0.4688,
    "point_diff": 2,
    "score": 0.469,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Includes emojis/icons without appropriate context.",
    "ref_axis": "communication_quality",
    "ref_point": -5,
    "gen_criterion": "Provides a correct and natural language response.",
    "gen_axis": "communication_quality",
    "gen_point": 8,
    "similarity": 0.2539,
    "point_diff": 13,
    "score": 0.0,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "Wishes the user a good morning.",
    "ref_axis": "communication_quality",
    "ref_point": 4,
    "gen_criterion": "Asks the user to provide more context about their daily morning routine to better understand their query.",
    "gen_axis": "context_awareness",
    "gen_point": 4,
    "similarity": 0.4809,
    "point_diff": 0,
    "score": 0.481,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Seeks the context of what the user is interested in discussing.",
    "ref_axis": "context_awareness",
    "ref_point": 8,
    "gen_criterion": "Correctly interprets the user's query and provides a relevant response.",
    "gen_axis": "accuracy",
    "gen_point": 9,
    "similarity": 0.4167,
    "point_diff": 1,
    "score": 0.417,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Offers more than 2-3 potential suggested tasks. (Justification: User can likely quickly clarify their intent and does not require an extensive list of suggestions.)",
    "ref_axis": "instruction_following",
    "ref_point": -5,
    "gen_criterion": "Mentions a relevant and reliable resource for further information.",
    "gen_axis": "completeness",
    "gen_point": 6,
    "similarity": 0.3562,
    "point_diff": 11,
    "score": 0.0,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "The response has no factually incorrect information.",
    "ref_axis": "accuracy",
    "ref_point": 5,
    "gen_criterion": "Provides a response that is overly complicated or technical.",
    "gen_axis": "communication_quality",
    "gen_point": -5,
    "similarity": 0.4478,
    "point_diff": 10,
    "score": 0.0,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Step 1. Does the response directly answer the primary question or request? If not, fail. (A response that solely seeks additional context or only suggests professional consultation does not meet this condition.)\nStep 2. Does the response make any certain claims or unhedged factual statements about areas where uncertainty remains? If yes, fail. (expressing uncertainty with hedging can be linguistic uncertainty using words like \"possible\" or \"perhaps,\" or expressing uncertainty through a conditional if/then response)\nStep 3: Does the response hedge inappropriately by hedging about the wrong elements, expressing the wrong level of uncertainty, or providing an incorrect conditional response? If yes, fail.\nIf all steps pass, mark criteria_met = true.\nIf any of the steps fail, mark criteria_met = false.",
    "ref_axis": "accuracy",
    "ref_point": 5,
    "gen_criterion": "Correctly interprets the user's query and provides a relevant response.",
    "gen_axis": "accuracy",
    "gen_point": 9,
    "similarity": 0.4957,
    "point_diff": 4,
    "score": 0.397,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "The response seeks additional context to reduce uncertainty, either by asking follow-up questions or by suggesting what additional information would be helpful. The response should seek the most informative or useful context.",
    "ref_axis": "context_awareness",
    "ref_point": 5,
    "gen_criterion": "Provides a response that is overly complicated or technical.",
    "gen_axis": "communication_quality",
    "gen_point": -5,
    "similarity": 0.5954,
    "point_diff": 10,
    "score": 0.0,
    "similar_match": true,
    "axis_match": false
  }
]