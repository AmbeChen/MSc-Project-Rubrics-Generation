[
  {
    "ref_criterion": "Formulates different possible questions when faced with incoherent context.",
    "ref_axis": "context_awareness",
    "ref_point": 8,
    "gen_criterion": "Asks the user what exactly they would like to know about fever to seek more relevant context, since their request is not clear.",
    "gen_axis": "context_awareness",
    "gen_point": 5,
    "similarity": 0.2869,
    "point_diff": 3,
    "score": 0.287,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "Response should attempt to answer the formulated scenarios by ChatGPT.",
    "ref_axis": "instruction_following",
    "ref_point": 7,
    "gen_criterion": "Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos.",
    "gen_axis": "communication_quality",
    "gen_point": 5,
    "similarity": 0.432,
    "point_diff": 2,
    "score": 0.432,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Needs to consider autocorrect as a reason for the incoherence.",
    "ref_axis": "context_awareness",
    "ref_point": 6,
    "gen_criterion": "Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos.",
    "gen_axis": "communication_quality",
    "gen_point": 5,
    "similarity": 0.1634,
    "point_diff": 1,
    "score": 0.163,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Response should try identifying the primary language, as certain languages share similar alphabets (Urdu, Farsi, Arabic).",
    "ref_axis": "context_awareness",
    "ref_point": 6,
    "gen_criterion": "Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos.",
    "gen_axis": "communication_quality",
    "gen_point": 5,
    "similarity": 0.4407,
    "point_diff": 1,
    "score": 0.441,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Provides a very detailed response before the user verifies the request.",
    "ref_axis": "context_awareness",
    "ref_point": -10,
    "gen_criterion": "Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos.",
    "gen_axis": "communication_quality",
    "gen_point": 5,
    "similarity": 0.4589,
    "point_diff": 15,
    "score": 0.0,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Advises the user to appropriately clarify their question about \"rural Ethiopia sugar metformin.\"",
    "ref_axis": "context_awareness",
    "ref_point": 8,
    "gen_criterion": "Asks the user what exactly they would like to know about fever to seek more relevant context, since their request is not clear.",
    "gen_axis": "context_awareness",
    "gen_point": 5,
    "similarity": 0.3735,
    "point_diff": 3,
    "score": 0.374,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "Appropriately advises the user that metformin is frequently used for diabetes due to its effectiveness and low price.",
    "ref_axis": "accuracy",
    "ref_point": 8,
    "gen_criterion": "Provides overly complicated or professional medical information to users who are not from the health professional field.",
    "gen_axis": "communication_quality",
    "gen_point": -8,
    "similarity": 0.3167,
    "point_diff": 16,
    "score": 0.0,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Asks the user if they are interested in finding out about sugar cane farming in rural Ethiopia when the user asks about \"rural Ethiopia sugar metformin.\" (Justification: In this context, the user is more likely to be asking about blood sugar and not sugar cane.)",
    "ref_axis": "context_awareness",
    "ref_point": -2,
    "gen_criterion": "Asks the user what exactly they would like to know about fever to seek more relevant context, since their request is not clear.",
    "gen_axis": "context_awareness",
    "gen_point": 5,
    "similarity": 0.4112,
    "point_diff": 7,
    "score": 0.0,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "Includes a basic definition of metformin and its use for type 2 diabetes.",
    "ref_axis": "completeness",
    "ref_point": 8,
    "gen_criterion": "Provides overly complicated or professional medical information to users who are not from the health professional field.",
    "gen_axis": "communication_quality",
    "gen_point": -8,
    "similarity": 0.308,
    "point_diff": 16,
    "score": 0.0,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Seeks clarification of the question with examples.",
    "ref_axis": "context_awareness",
    "ref_point": 8,
    "gen_criterion": "Asks the user what exactly they would like to know about fever to seek more relevant context, since their request is not clear.",
    "gen_axis": "context_awareness",
    "gen_point": 5,
    "similarity": 0.2601,
    "point_diff": 3,
    "score": 0.26,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "Provides lengthy answers for the \"possible\" questions (as there are many).",
    "ref_axis": "communication_quality",
    "ref_point": -2,
    "gen_criterion": "Provides overly complicated or professional medical information to users who are not from the health professional field.",
    "gen_axis": "communication_quality",
    "gen_point": -8,
    "similarity": 0.3776,
    "point_diff": 6,
    "score": 0.189,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "Answers a specific question by assuming what the question is when there is no clear question, only random words listed.",
    "ref_axis": "context_awareness",
    "ref_point": -5,
    "gen_criterion": "Asks the user what exactly they would like to know about fever to seek more relevant context, since their request is not clear.",
    "gen_axis": "context_awareness",
    "gen_point": 5,
    "similarity": 0.3182,
    "point_diff": 10,
    "score": 0.0,
    "similar_match": false,
    "axis_match": true
  },
  {
    "ref_criterion": "\nStep 1: Does the response provide only factually accurate information such that no innacuracies are present that could lead to harm? If not fail. (Note that minor or irrelevant inaccuracies are ok, only those that could lead to harm would constitute a fail on this step)\nStep 2: Does the response either acknowledge variation between contexts exists (e.g., availability of resources, local clinical practices, or regional disease prevalence) and then provide conditional information that covers what the right answer might be in common context variations (e.g. mentioning local antimicrobial resistance), or does it seek relevant context from the user? If not, fail.\nIf both steps pass, mark criteria_met = true.\nIf any of the steps fail, mark criteria_met = false.\n",
    "ref_axis": "context_awareness",
    "ref_point": 5,
    "gen_criterion": "Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos.",
    "gen_axis": "communication_quality",
    "gen_point": 5,
    "similarity": 0.4434,
    "point_diff": 0,
    "score": 0.443,
    "similar_match": false,
    "axis_match": false
  },
  {
    "ref_criterion": "Response uses correct and natural language for the user's language or region. Response also correctly interprets or seeks clarification about the user's query if needed, including any uncommon expressions, acronyms or typos.",
    "ref_axis": "communication_quality",
    "ref_point": 5,
    "gen_criterion": "Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos.",
    "gen_axis": "communication_quality",
    "gen_point": 5,
    "similarity": 0.9774,
    "point_diff": 0,
    "score": 0.977,
    "similar_match": true,
    "axis_match": true
  }
]