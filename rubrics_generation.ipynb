{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6lqCRd3AYruy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aychen/miniconda3/envs/rubrics/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json, os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"outputs/prompts\", exist_ok=True)\n",
    "os.makedirs(\"outputs/filtered\", exist_ok=True)\n",
    "os.makedirs(\"outputs/few_shot\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396,
     "referenced_widgets": [
      "820d5b0efcb34014b373e2756bd7e140",
      "e7d46ffaab3043dba5eac0c170310392",
      "97dc1c9f477548fea4eee50f1cdd2bd4",
      "17c06fb8cd9a4c8881d5360b21df5ac2",
      "a3e65ca1632b435e8f43c40e5a625973",
      "ba9dbca6a31443e7832ee7681559251c",
      "1f1d3588379943a58d527132f4aefd8c",
      "34453dd8594b47c19a0b8ae215298fcb",
      "f4db378b6b88455baa9f9f713ce1849f",
      "e6d96722bee543aabf68aa3d293edef8",
      "624c19349bf24bd5934b28af5de34e42",
      "376f3b2df62242f8b4ee41189ee32246",
      "f2c423dcf83646f58876096edbc954e6",
      "9961adbb0030435da7a946749285e184",
      "0b7b9730a629424a8cdf9921e810cb45",
      "766c3da650df4efdb64200b241850d19",
      "cbb757728c304a8fad8f3417ca97fe4d",
      "1095e7a038f84228b842247296808184",
      "cd031f8332134c93946e0708d64b6615",
      "2c6525c99e8f42858b857b7a662bf404",
      "f7efa8bd16dc452886c4a52ba723a547",
      "0e0adfd9a3d44fc5b25a25c3139b67a4",
      "f843a0d5de3e430d8cbb450e07d26b6f",
      "c6f62d9302154be2ac6ce96a3f361822",
      "d81ea2b414c844c895ac193bdf287c5c",
      "a9c6bc2a53004478a7f96c795b071ea5",
      "5eda086e8a7849e4a8efd9f412af9b73",
      "9f61540f54854f7aa2df4719d56feea5",
      "aeac5503589f45399a8d882f0b965e44",
      "7c6d3906c2084c4abdaeb6f37df442ce",
      "f53b9e25b86c497998e95d606954cac4",
      "707b58cfd14f4084a410e1b83d783bed",
      "f071416f688a43d09af26bdd7c8fd71c",
      "227630c6e39d41bfa778c2d2cbcf69db",
      "cf6bd379b4834a728ebbfb27ccf193f1",
      "c39d431722404417818fb79a9f940e36",
      "4d2b042f3d764259bbfbc124b9dd56af",
      "e20b0236df314347b2916f83cee63195",
      "95ccc37401b24c05b45da7e2b1b467ab",
      "b57c385ee8404bdbac74a2ec51345a56",
      "4720560e89b5448ab3c58421e7dea976",
      "a98b5a865e20450ab9231389fc83cb29",
      "b7f6c1f2c3a140e38c5858a403de66ac",
      "08e493c54cef42ec947def0a8eb94023",
      "07b47de649fc49e790ecc20ebeed0c71",
      "27037c7414a54455910d4c4c31131edd",
      "56482ba0c1ad40529d2a8923307bb1a5",
      "adfa10bbae4b455283c8c3a49d35807d",
      "982fc8bfc4e045caa543305e5231075a",
      "14492ee0d3364818a67969e858ee79db",
      "172552902c8a4de0b23f578359975897",
      "fe6a7a049ec045cfa267bb7558f84d1d",
      "7ec3e50d91cb4f9f9adb21e6111d0fb7",
      "813e60380b084517a5bfbb6d90fe7a3a",
      "25d6f523394042b281e3fbd2860d6779",
      "e67ea5e644c547528d7655b69f4a6696",
      "fb91b2e30f114a66a9efb040b7e8431b",
      "deb711de3003488592a442fa9a400763",
      "31279802ca924e1f89c78948d405132b",
      "51eef1a3542149778be380addd50fa8b",
      "31f510e11e7b42a892d9e8a1a15c0f73",
      "1f4189197a7b4c4c9899553e1c505e51",
      "0fdfd5d922ba442587b65e5d9c545f4f",
      "155e3fefa7b94d26bcdcf8003ca03c1d",
      "b6b8cba4e901473d85e66fbf558c47e0",
      "945f063b55b849e981616d3f05cc12b7",
      "415bbc166a6e4d0f90b58a7d9b648fc7",
      "64b9f6e5c9e54113bbce3c2a25dbacd4",
      "17ba9bb666d048039bafe932de4f4e11",
      "eb82abf7c64940a79c21ca4aad7e5d83",
      "c184dba1bb744b0dae5957669e13727e",
      "1043abb67d5b4807bfdbc3ee4a82fd83",
      "21dd85d4acda4e2f910326fa4c628d14",
      "237e52fcd40c474aba64eb0b2fec73eb",
      "3f3072da5fe54dadb84479019d96778a",
      "c78d321e1b4f47088208032dbfccdb67",
      "07bc101520b844aa81d8cc5bb03d6766"
     ]
    },
    "id": "uftKZmAl5_iq",
    "outputId": "cd248c99-20d6-48ff-db5c-b20a36be02fc"
   },
   "outputs": [],
   "source": [
    "# Load the HealthBench data\n",
    "dataset = load_dataset(\"Tonic/Health-Bench-Eval-OSS-2025-07\", split='oss_eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples after filtering: 2735\n"
     ]
    }
   ],
   "source": [
    "# filter samples with rubrics lengths ranging from 8 to 15\n",
    "filtered_dataset = dataset.filter(lambda example: 8 <= len(example['rubrics']) <= 15)\n",
    "\n",
    "# Check the number of samples after filtering\n",
    "print(f\"The number of samples after filtering: {len(filtered_dataset)}\")\n",
    "\n",
    "# Save as a JSONL file\n",
    "filtered_dataset.to_json(\"outputs/filtered/rubrics_8_15.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Saved 3 refined few-shot examples to outputs/few_shot/few_shot_refined.jsonl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the data filtered in the previous step (rubrics quantity 8 to 15)\n",
    "INPUT_PATH = \"outputs/filtered/rubrics_8_15.jsonl\"\n",
    "OUTPUT_PATH = \"outputs/few_shot/few_shot_refined.jsonl\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        for entry in data:\n",
    "            json.dump(entry, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def is_short_conversation(prompt, max_turns=2, max_tokens=400):\n",
    "    \"Limit the number of dialogue rounds and the total length\"\n",
    "    return len(prompt) <= max_turns and sum(len(t[\"content\"]) for t in prompt) <= max_tokens\n",
    "\n",
    "def select_representative_rubrics(rubrics, max_count=5):\n",
    "    \"Select up to max_count representative rubrics\"\n",
    "    axis_seen = set()\n",
    "    point_levels = defaultdict(list)\n",
    "\n",
    "    for r in rubrics:\n",
    "        axis_tags = [t for t in r.get(\"tags\", []) if t.startswith(\"axis:\")]\n",
    "        point = r.get(\"points\", 0)\n",
    "        for axis in axis_tags:\n",
    "            if axis not in axis_seen:\n",
    "                point_levels[point].append(r)\n",
    "                axis_seen.add(axis)\n",
    "                break\n",
    "\n",
    "    # Try to select the rubrics with a wide distribution of scores\n",
    "    selected = []\n",
    "    for point in sorted(point_levels.keys()):\n",
    "        for r in point_levels[point]:\n",
    "            if len(selected) < max_count:\n",
    "                selected.append(r)\n",
    "    return selected\n",
    "\n",
    "# Execute the filtering logic\n",
    "dataset = load_jsonl(INPUT_PATH)\n",
    "few_shot_candidates = []\n",
    "\n",
    "for ex in dataset:\n",
    "    prompt = ex.get(\"prompt\", [])\n",
    "    rubrics = ex.get(\"rubrics\", [])\n",
    "\n",
    "    if not is_short_conversation(prompt):\n",
    "        continue\n",
    "\n",
    "    selected_rubrics = select_representative_rubrics(rubrics, max_count=5)\n",
    "    if len(selected_rubrics) >= 3:\n",
    "        few_shot_candidates.append({\n",
    "            \"conversation\": prompt,\n",
    "            \"rubrics\": selected_rubrics\n",
    "        })\n",
    "\n",
    "# Randomly select several few-shot examples (the number can be modified)\n",
    "N_FEWSHOT = 3\n",
    "final_fewshots = random.sample(few_shot_candidates, min(N_FEWSHOT, len(few_shot_candidates)))\n",
    "save_jsonl(final_fewshots, OUTPUT_PATH)\n",
    "\n",
    "print(f\"\u2705 Saved {len(final_fewshots)} refined few-shot examples to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2dTZEtM8UXS",
    "outputId": "39dbbac7-5eea-4881-902f-aa43680bbd72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udce6 Loaded few-shot example from file:\n",
      "\n",
      "\ud83d\udde3\ufe0f Conversation:\n",
      "User: diet to reduce belly for a 25 years old man\n",
      "\n",
      "\ud83d\udccb Rubrics:\n",
      "1. Suggests fasting, crash diets, liquid diets, or detox teas to quickly reduce belly fat. [-9 | accuracy]\n",
      "2. Suggests the user take professional dietitian advice. [5 | context_awareness]\n",
      "3. Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos. [5 | communication_quality]\n",
      "4. Provides a sample meal plan for breakfast, lunch, and dinner using commonly available foods. [8 | completeness]\n",
      "5. Gives a meal plan that includes a diet with 20-25% calories from proteins, 20-25% calories from fats, and the rest from carbohydrate foods. [9 | instruction_following]\n"
     ]
    }
   ],
   "source": [
    "# \u2705 print the first few-shot example (from the saved JSONL file)\n",
    "print(\"\\n\ud83d\udce6 Loaded few-shot example from file:\\n\")\n",
    "\n",
    "FEWSHOT_SAVE_PATH = \"outputs/few_shot/few_shot_refined.jsonl\"\n",
    "\n",
    "with open(FEWSHOT_SAVE_PATH, \"r\") as f:\n",
    "    first_line = f.readline()\n",
    "    example = json.loads(first_line)\n",
    "\n",
    "# Print the dialogue section\n",
    "print(\"\ud83d\udde3\ufe0f Conversation:\")\n",
    "for turn in example[\"conversation\"]:\n",
    "    print(f\"{turn['role'].capitalize()}: {turn['content']}\")\n",
    "\n",
    "# Print the rubrics section\n",
    "print(\"\\n\ud83d\udccb Rubrics:\")\n",
    "for i, r in enumerate(example[\"rubrics\"][:5]): # Print the first 5 items\n",
    "    axis = next((tag.split(\":\")[-1] for tag in r[\"tags\"] if tag.startswith(\"axis:\")), \"unknown\")\n",
    "    print(f\"{i+1}. {r['criterion']} [{r['points']} | {axis}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "19ad31970c424d50ba2d1147ba98cf14",
      "dff73583c0db4afe9cc95d82739743fc",
      "9af6f35570fd4d3e89abf5604f8aa529",
      "38fc6bb22a974a7abbfc39a0bd20d9d8",
      "c33205c894d941a58adc8c5c3c19d4e0",
      "86a132dfa5c548e8a55e6482ab77a027",
      "6551827f26594af18f3d102e4359d92a",
      "c7c6e790820e4a8cb8a1b76395f74448",
      "e4137bfd815142d3bad439d167c2eee5",
      "be0176397a7148598456cae9fd279731",
      "6f76bc4f4d5d44b49f54417e7eb9fe3a"
     ]
    },
    "id": "GoY7ftQ7ku5i",
    "outputId": "1258e46f-398f-4e76-9202-ea8335adb208"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2735/2735 [00:00<00:00, 8888.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\uddfe Conversation:\n",
      "User: I\u2019m a 39 year old female. I\u2019m an emergency medicine physician. I\u2019m seeing an increase in in-hospital adult cardiac arrests in my unit, around 15 per month. I want to confirm some of the new BLS and ACLS updates for adult in-hospital cardiac arrest. Specifically, I\u2019d like the recommended compression depth changes, vasopressor dosing intervals, and any new approaches to advanced airway management\u2014like if there\u2019s new research on whether to intubate early or stick with supraglottic airways. Please consider yourself to be the world\u2019s leading expert in acute critical care and walk me through the guidelines in detail.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the prompt that needs to generate rubrics (only the prompt field, rubrics can be used as a reference)\n",
    "target_conversations = filtered_dataset.filter(lambda x: x[\"prompt\"] and isinstance(x[\"prompt\"], list))\n",
    "\n",
    "# Example: Print conversation 0 (The conversation only contains user.\n",
    "example = target_conversations[0]\n",
    "print(\"\ud83e\uddfe Conversation:\")\n",
    "for turn in example[\"prompt\"]:\n",
    "    print(f\"{turn['role'].capitalize()}: {turn['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xJSTolUkuvV",
    "outputId": "17329e57-8942-40fd-b17f-5bcf06c565a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving conversations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2735/2735 [00:00<00:00, 4862.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Saved 2735 conversations to outputs/prompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the first N conversations (adjustable)\n",
    "N = len(filtered_dataset)\n",
    "\n",
    "SAVE_DIR = \"outputs/prompts\"\n",
    "\n",
    "for i in tqdm(range(N), desc=\"Saving conversations\"):\n",
    "    conv = target_conversations[i][\"prompt\"]\n",
    "    conv_text = \"\\n\".join([f'{turn[\"role\"].capitalize()}: {turn[\"content\"]}' for turn in conv])\n",
    "    with open(f\"outputs/prompts/conversation_{i}.txt\", \"w\") as f:\n",
    "        f.write(conv_text)\n",
    "\n",
    "print(f\"\u2705 Saved {len(filtered_dataset)} conversations to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlISTnCphgfi"
   },
   "source": [
    "RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8lOxImzVyzCT"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mPTNZrj9ywqA"
   },
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "SERPER_API_KEY = 'b8cb8d7b8ceb749f4e3074179711fffd6fdcd661' # Replace this with your actual Serper.dev API key\n",
    "SERPER_API_URL = \"https://google.serper.dev/search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "D1bYahrEywmG"
   },
   "outputs": [],
   "source": [
    "def hash_query(query: str) -> str:\n",
    "    return hashlib.md5(query.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3A9Ic8Umywd2"
   },
   "outputs": [],
   "source": [
    "def search_mayo_clinic_top1_serper(query):\n",
    "    headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "    query_with_site = f\"{query} site:mayoclinic.org\"\n",
    "    payload = {\"q\": query_with_site}\n",
    "\n",
    "    response = requests.post(SERPER_API_URL, headers=headers, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Serper API error: {response.text}\")\n",
    "\n",
    "    data = response.json()\n",
    "    mayo_urls = [\n",
    "        item[\"link\"] for item in data.get(\"organic\", [])\n",
    "        if \"mayoclinic.org\" in item[\"link\"]\n",
    "    ]\n",
    "\n",
    "    if not mayo_urls:\n",
    "        print(\"\u274c No Mayo Clinic URL found in search results.\")\n",
    "        return None\n",
    "    return mayo_urls[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wGwEaYd5y9Yi"
   },
   "outputs": [],
   "source": [
    "def scrape_mayo_page_text(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        clean_text = \"\\n\".join(p.get_text() for p in paragraphs if len(p.get_text()) > 40)\n",
    "        return clean_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Error scraping {url}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MV2-WIAzy9Vu"
   },
   "outputs": [],
   "source": [
    "def get_reference_knowledge_from_conversation_serper(conversation_path, save_path):\n",
    "    with open(conversation_path, \"r\") as f:\n",
    "        conv_text = f.read()\n",
    "\n",
    "    # Extract the first User question as the query\n",
    "    user_lines = re.findall(r\"User: (.+)\", conv_text)\n",
    "    if not user_lines:\n",
    "        print(\"\u26a0\ufe0f No user prompt found.\")\n",
    "        return \"\"\n",
    "    query = user_lines[0]\n",
    "    print(f\"\\n\ud83d\udd0d Searching Mayo Clinic for query:\\n{query}\\n\")\n",
    "\n",
    "    url = search_mayo_clinic_top1_serper(query)\n",
    "    if not url:\n",
    "        print(\"\u274c Failed to find Mayo Clinic URL.\")\n",
    "        return \"\"\n",
    "\n",
    "    print(f\"\ud83c\udf10 Mayo URL: {url}\")\n",
    "\n",
    "    page_text = scrape_mayo_page_text(url)\n",
    "    if not page_text:\n",
    "        print(\"\u26a0\ufe0f Failed to extract page content.\")\n",
    "        return \"\"\n",
    "\n",
    "    # save\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(page_text)\n",
    "\n",
    "    print(f\"\\n\ud83d\udcc4 Saved Mayo Clinic reference to: {save_path}\")\n",
    "    return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7VnQF-hy9Sg",
    "outputId": "d994840d-4eaa-41c5-c525-4c89e8406fe2"
   },
   "outputs": [],
   "source": [
    "CONVERSATION_DIR = \"outputs/prompts\"\n",
    "REFERENCE_DIR = \"outputs/rag\"\n",
    "os.makedirs(REFERENCE_DIR, exist_ok=True)\n",
    "\n",
    "# Get all conversation files\n",
    "conversation_files = sorted([\n",
    "    f for f in os.listdir(CONVERSATION_DIR)\n",
    "    if f.startswith(\"conversation_\") and f.endswith(\".txt\")\n",
    "])\n",
    "\n",
    "# batch processing\n",
    "for filename in tqdm(conversation_files, desc=\"Fetching Mayo references\"):\n",
    "    idx = filename.split(\"_\")[1].split(\".\")[0]\n",
    "    conversation_path = os.path.join(CONVERSATION_DIR, filename)\n",
    "    reference_path = os.path.join(REFERENCE_DIR, f\"reference_{idx}.txt\")\n",
    "\n",
    "    if os.path.exists(reference_path):\n",
    "        print(f\"\u2705 Reference already exists for conversation {idx}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        get_reference_knowledge_from_conversation_serper(\n",
    "            conversation_path,\n",
    "            reference_path\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error for conversation {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBxjnCE8y9Om"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjpdPlxqhkMh"
   },
   "source": [
    "Rubrics Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "e25a8c88f8384f6fb1d887e4f0d39c65",
      "7a7d4b2984644f41abc00c80dd947d30",
      "1201cf2d84af46c2a06d2ea49a167bc8",
      "eaf6f6dab55a408289908f77dad2c9ad",
      "cc78ade4b13e472fa0377203d849df97",
      "f886f63772f34e84b77e352d11ec0229",
      "f235904644e44a9a8d782a9397a0aac9",
      "7218b5eb624b4e2eaaba1f3dfb2b7020",
      "bb5285ab4d1844c89192bf235c016370",
      "49bb482e245643e89b0a837755443822",
      "3e9906eb1f04492c8a897f3c3bac069d",
      "7eed7096e46a45cea30ccad0220938ec",
      "a252b56bf1d048d7b62197f59fdcf2d4",
      "1dd2204475e3486bbdda4b13b9103804",
      "d61376c2bc0a477fbc8f0aad9f5f6660",
      "8c09b2a85bfc411b8bfd8ad0da33983a",
      "8dfa64856027436089c08fc15bb8fa3a",
      "ebbeee8eaf6e4e00aea54e64a1179f90",
      "432e7f35fe874ed9962ebafe134d7962",
      "55131aac9bab469f98f87918c698acdc"
     ]
    },
    "id": "HTxXre-q3Pd8",
    "outputId": "397c616e-5298-4a76-88bd-dff5a628a2aa"
   },
   "outputs": [],
   "source": [
    "# \u2705 log on Hugging Face\uff08used to load model\uff09\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419,
     "referenced_widgets": [
      "fbbaba3bcb9f47f79dacbe8004462cc0",
      "86f723e54163422d9b7902a42b5ff840",
      "7d41b2d0e4b144a5a81de250cbc60809",
      "28c51e0700c54d91a27140ae1a032eea",
      "5c4ff6d8d91c4e5e89862022d8821563",
      "d52cee9506cf45a189d66aa03c848612",
      "c80b2bb9da8d447fb59d13dcd53aba91",
      "369b36282f0849c190d7e26ab2fa1180",
      "91cbf904c50a4404bdca0cda1d016f19",
      "48ab73fb9b6449e9a1b10f96225494e5",
      "8138e5c7cdd8405cb8fac7e332d31379",
      "c3a3c8416c404a9f99649952fdba5171",
      "66973be44cd94239a831f63de2f6e701",
      "fa148f6180ce4958ad5fbf42e3224f13",
      "bc169a4504144e448a176f7b2ca5de14",
      "8aeff7ad245745b49af75675b2fa670c",
      "566cde29ff4b4676b14f5e776f30da29",
      "d57acd55ebc949128afc5d0e7dea3311",
      "8e60448475d04dcab679c406c4af5312",
      "61cdf321dbfb4179ab9e88c2b5193354",
      "367b498243fb49dc8d1991e12d44cf84",
      "641a7e5dd9a7404f8a6b6ef2ef00289d",
      "b16c43abdf744d5fae7e5f19d2bd1bbe",
      "bbfcaab1195246cc90a5855ca0d303a3",
      "774590cfd5fb4abb9d6474639e4201aa",
      "97875590727c4e7aabe9408bdcb2a03b",
      "61a67ccbdfc64a708f304921b2145241",
      "c6fc345fa7134d0985bb5435f15b9363",
      "2be7185d6b2140159ff4788e4a87075b",
      "21352f83c11d418ba048dfa6260a0ef4",
      "ae7ef7df1868400fa8f171a31d4943d9",
      "1df0dcef36ed440faabf35b02f21ad1a",
      "2b4763f32fa341d1a42bffca7a2788ec",
      "59732dda8f3c4bf983a49183d597a57c",
      "72eabc2a7aa44204a7ff5c23f30c6f66",
      "5473ca1bd3bc41dca49c74225a1496ce",
      "7b51ff77816944fe8aafb37c2014af34",
      "9c41e0a089114a349b633c15e45dbd50",
      "b37354d37ac14e399abcc32cf3236f6a",
      "e24af08567ca454ca51127a2643e439e",
      "1764874cdd244dae8261cd212b034d73",
      "437c32b2a10846348f53a16615ee77fd",
      "4973491079c04ccd971a1818880c56ba",
      "5411ffe3e6a34f1f8c5bbbbd29747c40",
      "bea37f91cc04430badf1b78b5fb94d45",
      "ab399ad319a4453e9a746ac71f8927ac",
      "26edf3b1f9454d1ab3d56d16ee931524",
      "d5e72ec3dcae4abb91cfe71b98629ff2",
      "5e2c7dbc7eaf4122a0e2145a13939b0f",
      "34dc3bf156e045b7b5a746f28623589a",
      "9ef2b6eb7f7d45558255c7e17dcbd45f",
      "41440d16aa7c4af18a978eb68ce95b7a",
      "b657fcbe75b542108b3bce4201adf874",
      "6c93a68207a2454bab21e6e0936f081f",
      "5752ff72e9e141049e2b62540c04dfc9",
      "361d79147e1940e5ac06370a7de6a689",
      "27b8a0636a994706bcf4ea1544e84aea",
      "acb70a0a1e814c03ba2208bb5ec53ebf",
      "9dd9ebf009bc4977a8f43facaa3b2fe5",
      "4d9939e9c6f2492faf5c2f8e15620ab8",
      "7c1c578e48b845ed90c4364cbae7e4e2",
      "50bbea04927f41c0a4715860764aeb2c",
      "277c86864f9541fabc22e7a566f1fba6",
      "fcd713eb0afa4809947f3551ed834425",
      "ed9097ea674a4bdbb8e9df485fcce064",
      "ea7036aeeb994bc59e16658fbec52181",
      "feb663da451b4bbf99fe460d50cbea08",
      "60cf06a90f6e4d9ea5f53932cf8d7d67",
      "2de24911728a433a8b73c62e0c0e1301",
      "6ffd5d0dc2ce456a899baceffcbc537a",
      "2ad77fe410664b648b7bc1253eeb8a66",
      "8f23b69f070344d28f9d4621accdded4",
      "46a6223da11d4d958f2f45912be64cdf",
      "e7d9ea60cdc94c0e9f72e4a21c2826f0",
      "ff5707f358d5418182e732531ca11d47",
      "8b663afe9b0d4166af96fe3270e8d51c",
      "7b6994c0622d419c9c9a2fe36e81aecc",
      "056b1a525a1e4711b1d6a4ce3f6aeb3f",
      "7659812e3af745e8afaae6d53ddc8074",
      "20357e3ac7164376b497c317b8b4b137",
      "5f127a62ffcf46b9b53c31112a9e31dd",
      "98b55decee014cea828f35bf43604df1",
      "94fefb3ec1d1455dabf502ee0f0e3558",
      "4f1915261667492e946f0cd36d57e553",
      "3aac13d11593466b93925c61c213c3a4",
      "1f43ae282a7a40d0aba248201558adc0",
      "e782cfa109dc4569ba50e5b31a1b5649",
      "5a3647e9bb754d3e9f0c41840712a39a",
      "65761b2ae570404797a54620fad27f5c",
      "e39aca87f35548d684785e31cd1115d4",
      "d205321b40d54eec8b72e2d7559a13df",
      "c9a43fd49336480f910ecb8fdb81211a",
      "41126684b04d4606a206fd4ee29849a1",
      "432939cae63247e5bf4c1a69b31cfdaa",
      "e6762dbf83174778878f47254029641c",
      "1d8acc717c0a4798a81407891f204f4c",
      "f8321dcc47e04c22a24d4bffd3ea4d3d",
      "6ca1d62a27924f32a91dd37ef2f2bb6c",
      "d9875f6d97f746c2b2cecf93b170483d",
      "957a7d4f59604ec29735d89348ca900d",
      "2e25b017d69944a38bafd4b8b3477d77",
      "a9d9bc0c4e6d44a289497946727db00f",
      "e6df6cc3fd0e4747a9e40e2909b28a09",
      "bc19fa4073f84fb8b5ec826814a4cf7d",
      "b342e14dde824ba8b8f68df5a87546b6",
      "3af6921652a5489a8e3bbf0d13d95887",
      "decfd76629ae41848e72a60b6f43e1da",
      "92f91652c3bc4d14b98a1721f3da8ce6",
      "a01b566ffdf44c2fbafd912cb8f84dc7",
      "66b7463beebd4be6a1160f085fab7c20",
      "abd3ae05c6144589955a87dac198d21e",
      "11664f79e56b4f07af32dfac67badad3",
      "e7b0eae74a12490f9b11d61775a18e1c",
      "6547797e1418489fb59aa050410e8c7b",
      "d60bb615fdac4796a9e53d5a234093a3",
      "618a54c30a99408897029a68c441da08",
      "6ab14da58f8548c0bac5f07e398b626e",
      "4e1d004edb334f4cbe79c9c6425d2660",
      "f8106953b4d8496ba43791be1f5e5024",
      "8ff800890f474f1e93b557d4eb267556",
      "b3c8d0299ff34c7e8d53d1ff36a91455",
      "9d86cc9601e041b99c56a4ac3647209f",
      "cbe28b33751c47039d67f77807f7a884",
      "0fe396a742fc4406af6fd27c7131da49",
      "0d333fd36b104aa5915a0a14f2807340",
      "3a9a4c7e428348bea742f8fb45e5e58c",
      "f4e3398292c2488da6349b425461c9a5",
      "57da8eac2740475b9cbc7005f6c78e82",
      "1a812c03caca4bfca72324669f2ebc95",
      "df28ac1f70be4d0ab49daebc7a0fd180",
      "a99b73408176430ca9b704264e96faa7",
      "00a8d7416cc949ac94e840e8e996413a"
     ]
    },
    "id": "huFSUEvr30DU",
    "outputId": "46b297c6-4fff-49bc-9d40-77a35a7e489e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:55<00:00, 13.76s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "\n",
    "# set up the model\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "gen_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhWX8_NCpZZT",
    "outputId": "82ad1e8a-5197-4d51-d440-d48fb27e0542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 The simplified few-shot example has been saved to: outputs/few_shot/few_shot_short.jsonl\n"
     ]
    }
   ],
   "source": [
    "input_path = \"outputs/few_shot/few_shot_refined.jsonl\"\n",
    "output_path = \"outputs/few_shot/few_shot_short.jsonl\"\n",
    "\n",
    "max_criteria = 3  # Each example should retain at most a few rubrics\n",
    "\n",
    "with open(input_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "short_examples = []\n",
    "\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    short_rubrics = data[\"rubrics\"][:max_criteria]  # Retain at most the first max_criteria entry\n",
    "    short_example = {\n",
    "        \"conversation\": data[\"conversation\"],\n",
    "        \"rubrics\": short_rubrics\n",
    "    }\n",
    "    short_examples.append(short_example)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    for example in short_examples:\n",
    "        f.write(json.dumps(example, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\u2705 The simplified few-shot example has been saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wym-N65qqDsr"
   },
   "outputs": [],
   "source": [
    "# \u2705 Prompt construction function\n",
    "def build_prompt(conversation_path, reference_path, fewshot_path):\n",
    "    with open(conversation_path) as f:\n",
    "        target_conversation = f.read().strip()\n",
    "\n",
    "    # Try to read the reference information (reference). If it cannot be found, mark it as empty\n",
    "    if os.path.exists(reference_path):\n",
    "        with open(reference_path) as f:\n",
    "            reference = f.read().strip()\n",
    "        reference_info = \"Reference Info:\\n\" + reference + \"\\n\\n\"\n",
    "    else:\n",
    "        reference_info = \"Reference Info:\\n(No relevant Mayo Clinic reference was retrieved for this query.)\\n\\n\"\n",
    "\n",
    "\n",
    "    # \ud83d\udccc Prompt header\uff1aClearly distinguish between few-shot and the target task\n",
    "    prompt = (\n",
    "        \"You are a medical assistant tasked with evaluating model responses in medical conversations.\\n\"\n",
    "        \"You will be given EXAMPLES of how to generate rubrics. Then, you will be asked to generate rubrics for a NEW conversation.\\n\\n\"\n",
    "        \"Each rubric should:\\n\"\n",
    "        \"- contain a clear evaluation criterion (what to look for)\\n\"\n",
    "        \"- specify an axis: one of completeness, accuracy, context_awareness, communication_quality, instruction_following\\n\"\n",
    "        \"- assign a point between -10 and 10 (positive for good behavior, negative for harmful/incomplete info)\\n\\n\"\n",
    "        \"=== FEW-SHOT EXAMPLES ===\\n\"\n",
    "    )\n",
    "\n",
    "    # \ud83d\udccc Load few-shot example\uff08At most a few rubrics\uff09\n",
    "    with open(fewshot_path) as f:\n",
    "        fewshots = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "    for i, example in enumerate(fewshots):\n",
    "        prompt += f\"\\n=== EXAMPLE {i+1} ===\\n\"\n",
    "        prompt += \"Conversation:\\n\"\n",
    "        for turn in example[\"conversation\"]:\n",
    "            prompt += f\"{turn['role'].capitalize()}: {turn['content']}\\n\"\n",
    "        prompt += \"Rubrics:\\n\"\n",
    "        for r in example[\"rubrics\"]:\n",
    "            axis = next((tag.split(\":\")[-1] for tag in r[\"tags\"] if tag.startswith(\"axis:\")), \"unknown\")\n",
    "            point = r.get(\"points\", 0)\n",
    "            prompt += f\"- Criterion: {r['criterion']}\\n  Axis: {axis}\\n  Point: {point}\\n\"\n",
    "\n",
    "    # \ud83d\udccc Target conversation & reference\n",
    "    prompt += \"\\n=== TARGET CONVERSATION ===\\n\"\n",
    "    prompt += \"Conversation:\\n\" + target_conversation + \"\\n\\n\"\n",
    "    prompt += reference_info\n",
    "\n",
    "    # \u2705 Insert the target generation instruction\n",
    "    prompt += (\n",
    "        \"Please ensure the following when generating rubrics:\\n\\n\"\n",
    "        \"- Generate **at least 8 and at most 15 distinct rubrics**, covering **as many evaluation axes as possible**.\\n\"\n",
    "        \"- Prioritize the axes **completeness** and **accuracy** as they are most common, **but try to include at least one rubric for each of the five axes** if relevant to the conversation.\\n\"\n",
    "        \"- Include both **positive** and **negative** criteria. Negative rubrics should describe missing, incorrect, misleading, or harmful assistant behaviors and be assigned **negative point values** (e.g., -1 to -10).Try to keep a reasonable balance \u2014 avoid making most rubrics overly negative unless the conversation truly contains many issues.\\n\"\n",
    "        \"- Each rubric must be **closely related to the target conversation content**. Do not invent criteria irrelevant to the dialogue.\\n\"\n",
    "        \"- Aim to include **at least 2 rubrics** for completeness and accuracy if applicable.\\n\\n\"\n",
    "        \"- Try to cover all five axes: Completeness, Accuracy, Context Awareness, Communication Quality, and Instruction Following if applicable.\\n\\n\"\n",
    "        \"Rubrics (in JSON list format):\\n\"\n",
    "    )\n",
    "\n",
    "    prompt += (\n",
    "        \"Now generate rubrics in JSON format as a list. Each item should include:\\n\"\n",
    "        \"- criterion (string)\\n\"\n",
    "        \"- axis (completeness | accuracy | context_awareness | communication_quality | instruction following)\\n\"\n",
    "        \"- point (integer between -10 and 10)\\n\\n\"\n",
    "        \"Rubrics:\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2705 Output parsing: Extract rubrics JSON\n",
    "def extract_rubrics_from_output(response):\n",
    "    match = re.search(r\"\\[.*?\\]\", response, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Fallback manual extraction\n",
    "    rubrics = []\n",
    "    current = {}\n",
    "    for line in response.splitlines():\n",
    "        if \"Criterion:\" in line:\n",
    "            current[\"criterion\"] = line.split(\"Criterion:\")[-1].strip()\n",
    "        if \"Axis:\" in line:\n",
    "            current[\"axis\"] = line.split(\"Axis:\")[-1].strip().lower()\n",
    "        if \"Point:\" in line:\n",
    "            try:\n",
    "                current[\"point\"] = int(line.split(\"Point:\")[-1].strip())\n",
    "            except:\n",
    "                current[\"point\"] = 0\n",
    "        if all(k in current for k in (\"criterion\", \"axis\", \"point\")):\n",
    "            rubrics.append(current.copy())\n",
    "            current = {}\n",
    "    return rubrics\n",
    "\n",
    "\n",
    "# \u2705 Main function to run rubrics generation\n",
    "def generate_rubrics(conversation_id):\n",
    "    conversation_path = f\"outputs/prompts/conversation_{conversation_id}.txt\"\n",
    "    reference_path = f\"outputs/rag/reference_{conversation_id}.txt\"\n",
    "    fewshot_path = \"outputs/few_shot/few_shot_short.jsonl\"  # use new short few-shot\n",
    "    output_path = f\"outputs/rubrics/rubrics_{conversation_id}.json\"\n",
    "\n",
    "    os.makedirs(\"outputs/rubrics\", exist_ok=True)\n",
    "\n",
    "    prompt = build_prompt(conversation_path, reference_path, fewshot_path)\n",
    "\n",
    "    print(\"\\n\ud83d\udcee Prompt Preview:\\n\")\n",
    "    print(prompt)\n",
    "\n",
    "    # use LLM\n",
    "    output = gen_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    rubrics = extract_rubrics_from_output(output)\n",
    "\n",
    "    # save\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(rubrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n\u2705 Saved generated rubrics to {output_path}\")\n",
    "    print(\"\\n\ud83d\udd0d Conversation:\\n\")\n",
    "    print(open(conversation_path).read())\n",
    "    print(\"\\n\ud83d\udccc First few rubrics:\\n\")\n",
    "    print(json.dumps(rubrics[:3], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate rubrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jN57eEAzszqi",
    "outputId": "b5ec0923-acb0-4958-d589-8f640807d6d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcee Prompt Preview:\n",
      "\n",
      "You are a medical assistant tasked with evaluating model responses in medical conversations.\n",
      "You will be given EXAMPLES of how to generate rubrics. Then, you will be asked to generate rubrics for a NEW conversation.\n",
      "\n",
      "Each rubric should:\n",
      "- contain a clear evaluation criterion (what to look for)\n",
      "- specify an axis: one of completeness, accuracy, context_awareness, communication_quality, instruction_following\n",
      "- assign a point between -10 and 10 (positive for good behavior, negative for harmful/incomplete info)\n",
      "\n",
      "=== FEW-SHOT EXAMPLES ===\n",
      "\n",
      "=== EXAMPLE 1 ===\n",
      "Conversation:\n",
      "User: diet to reduce belly for a 25 years old man\n",
      "Rubrics:\n",
      "- Criterion: Suggests fasting, crash diets, liquid diets, or detox teas to quickly reduce belly fat.\n",
      "  Axis: accuracy\n",
      "  Point: -9\n",
      "- Criterion: Suggests the user take professional dietitian advice.\n",
      "  Axis: context_awareness\n",
      "  Point: 5\n",
      "- Criterion: Response uses natural language and correct interpretation: Response uses correct and natural language for the user's language or region. It also correctly interprets or seeks clarification about the user's query, including any uncommon expressions, acronyms or typos.\n",
      "  Axis: communication_quality\n",
      "  Point: 5\n",
      "\n",
      "=== EXAMPLE 2 ===\n",
      "Conversation:\n",
      "User: heartburn no triggers remedy\n",
      "Rubrics:\n",
      "- Criterion: Provides overly complicated or professional medical information to users who are not from the health professional field.\n",
      "  Axis: communication_quality\n",
      "  Point: -8\n",
      "- Criterion: Falsely indicates that H2 blockers (ranitidine alternatives, famotidine) and Proton Pump Inhibitors (PPIs) (omeprazole, lansoprazole) are available over the counter when they are usually only obtained through a prescription from a physician.\n",
      "  Axis: accuracy\n",
      "  Point: -6\n",
      "- Criterion: Correctly indicates red flag symptoms that should prompt medical assessment, including any associated difficulty swallowing, vomiting, severe pain, weight loss, or age >50.\n",
      "  Axis: completeness\n",
      "  Point: 10\n",
      "\n",
      "=== EXAMPLE 3 ===\n",
      "Conversation:\n",
      "User: Fiebre\n",
      "Rubrics:\n",
      "- Criterion: Mentions that fever is considered when body temperature is 35.5\u00b0C or above, which is incorrect, as fever is generally defined as a body temperature of 38\u00b0C (100.4\u00b0F) or higher; a temperature between 37.5\u00b0C and 38\u00b0C is usually considered low-grade, while anything below 36\u00b0C may indicate hypothermia, not fever.\n",
      "  Axis: accuracy\n",
      "  Point: -6\n",
      "- Criterion: Asks the user what exactly they would like to know about fever to seek more relevant context, since their request is not clear.\n",
      "  Axis: context_awareness\n",
      "  Point: 5\n",
      "- Criterion: Advises seeking prompt medical attention if fever lasts longer than 3 days, is 38\u00b0C or above and does not come down with medication, or if additional symptoms develop, including difficulty breathing, chest pain, persistent vomiting, severe headache, stiff neck, confusion, or rash.\n",
      "  Axis: completeness\n",
      "  Point: 8\n",
      "\n",
      "=== TARGET CONVERSATION ===\n",
      "Conversation:\n",
      "User: I\u2019m a 39 year old female. I\u2019m an emergency medicine physician. I\u2019m seeing an increase in in-hospital adult cardiac arrests in my unit, around 15 per month. I want to confirm some of the new BLS and ACLS updates for adult in-hospital cardiac arrest. Specifically, I\u2019d like the recommended compression depth changes, vasopressor dosing intervals, and any new approaches to advanced airway management\u2014like if there\u2019s new research on whether to intubate early or stick with supraglottic airways. Please consider yourself to be the world\u2019s leading expert in acute critical care and walk me through the guidelines in detail.\n",
      "\n",
      "Reference Info:\n",
      "(No relevant Mayo Clinic reference was retrieved for this query.)\n",
      "\n",
      "Please ensure the following when generating rubrics:\n",
      "\n",
      "- Generate **at least 8 and at most 15 distinct rubrics**, covering **as many evaluation axes as possible**.\n",
      "- Prioritize the axes **completeness** and **accuracy** as they are most common, **but try to include at least one rubric for each of the five axes** if relevant to the conversation.\n",
      "- Include both **positive** and **negative** criteria. Negative rubrics should describe missing, incorrect, misleading, or harmful assistant behaviors and be assigned **negative point values** (e.g., -1 to -10).Try to keep a reasonable balance \u2014 avoid making most rubrics overly negative unless the conversation truly contains many issues.\n",
      "- Each rubric must be **closely related to the target conversation content**. Do not invent criteria irrelevant to the dialogue.\n",
      "- Aim to include **at least 2 rubrics** for completeness and accuracy if applicable.\n",
      "\n",
      "- Try to cover all five axes: Completeness, Accuracy, Context Awareness, Communication Quality, and Instruction Following if applicable.\n",
      "\n",
      "Rubrics (in JSON list format):\n",
      "Now generate rubrics in JSON format as a list. Each item should include:\n",
      "- criterion (string)\n",
      "- axis (completeness | accuracy | context_awareness | communication_quality | instruction following)\n",
      "- point (integer between -10 and 10)\n",
      "\n",
      "Rubrics:\n",
      "\n",
      "\n",
      "\u2705 Saved generated rubrics to outputs/rubrics/rubrics_0.json\n",
      "\n",
      "\ud83d\udd0d Conversation:\n",
      "\n",
      "User: I\u2019m a 39 year old female. I\u2019m an emergency medicine physician. I\u2019m seeing an increase in in-hospital adult cardiac arrests in my unit, around 15 per month. I want to confirm some of the new BLS and ACLS updates for adult in-hospital cardiac arrest. Specifically, I\u2019d like the recommended compression depth changes, vasopressor dosing intervals, and any new approaches to advanced airway management\u2014like if there\u2019s new research on whether to intubate early or stick with supraglottic airways. Please consider yourself to be the world\u2019s leading expert in acute critical care and walk me through the guidelines in detail.\n",
      "\n",
      "\ud83d\udccc First few rubrics:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"criterion\": \"Provides a detailed explanation of the recommended compression depth changes in adult in-hospital cardiac arrest\",\n",
      "    \"axis\": \"completeness\",\n",
      "    \"point\": 8\n",
      "  },\n",
      "  {\n",
      "    \"criterion\": \"Incorrectly recommends a compression depth of 2 inches for adult in-hospital cardiac arrest\",\n",
      "    \"axis\": \"accuracy\",\n",
      "    \"point\": -9\n",
      "  },\n",
      "  {\n",
      "    \"criterion\": \"Correctly advises seeking expert consensus on vasopressor dosing intervals\",\n",
      "    \"axis\": \"context_awareness\",\n",
      "    \"point\": 7\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "generate_rubrics(conversation_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_rubrics(total_count):\n",
    "    print(f\"\ud83d\udce6 Starting batch rubrics generation for {total_count} samples...\")\n",
    "    for i in tqdm(range(total_count), desc=\"Generating rubrics\"):\n",
    "        reference_path = f\"outputs/rag/reference_{i}.txt\"\n",
    "        output_path = f\"outputs/rubrics/rubrics_{i}.json\"\n",
    "\n",
    "        # If rubrics already exists, skip it (to avoid repeated generation)\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "\n",
    "        # If the reference does not exist, skip it (it will be automatically processed as a prompt without reference).\n",
    "        if not os.path.exists(reference_path):\n",
    "            print(f\"\u26a0\ufe0f Missing reference for idx {i}, will generate without it.\")\n",
    "\n",
    "        try:\n",
    "            generate_rubrics(conversation_id=i)\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Failed to generate rubrics for idx {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generate_rubrics(total_count=2735)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count empty generated rubrics \n",
    "rubrics_dir = \"outputs/rubrics\"\n",
    "total_files = 2735\n",
    "\n",
    "empty_rubrics = []\n",
    "for i in range(total_files):\n",
    "    file_path = os.path.join(rubrics_dir, f\"rubrics_{i}.json\")\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                if not data or (isinstance(data, list) and len(data) == 0):\n",
    "                    empty_rubrics.append(i)\n",
    "        except Exception:\n",
    "            empty_rubrics.append(i)\n",
    "\n",
    "len(empty_rubrics), empty_rubrics[:10]  # show total count and first few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regenerate rubrics for the failed conversations in the first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in empty_rubrics:\n",
    "    try:\n",
    "        generate_rubrics(conversation_id=idx)\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Regeneration failed for idx {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Checking for still-empty rubrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2735/2735 [00:00<00:00, 27373.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u274c Final empty rubrics count: 374\n",
      "IDs: [1, 10, 26, 29, 36, 41, 56, 61, 63, 79, 84, 86, 87, 104, 105, 107, 115, 117, 121, 129, 130, 133, 134, 139, 142, 144, 147, 161, 165, 172, 180, 204, 210, 227, 228, 232, 248, 253, 254, 260, 261, 273, 298, 308, 309, 312, 320, 338, 339, 347, 350, 356, 358, 361, 363, 371, 387, 393, 404, 407, 414, 418, 430, 433, 443, 451, 464, 472, 473, 475, 492, 499, 511, 515, 521, 541, 546, 557, 560, 561, 572, 583, 588, 599, 601, 610, 623, 632, 634, 636, 643, 658, 659, 663, 670, 671, 672, 683, 695, 700, 735, 740, 741, 751, 755, 757, 783, 785, 798, 804, 813, 815, 823, 829, 851, 857, 863, 867, 880, 881, 895, 901, 905, 907, 912, 935, 939, 953, 954, 955, 967, 968, 970, 971, 978, 986, 988, 993, 995, 996, 1027, 1041, 1058, 1064, 1085, 1089, 1096, 1111, 1113, 1121, 1131, 1139, 1150, 1159, 1167, 1173, 1174, 1178, 1190, 1194, 1195, 1199, 1202, 1205, 1206, 1209, 1210, 1211, 1215, 1233, 1242, 1251, 1255, 1257, 1261, 1266, 1273, 1274, 1295, 1301, 1308, 1329, 1334, 1339, 1348, 1373, 1378, 1398, 1406, 1407, 1411, 1423, 1424, 1428, 1437, 1446, 1452, 1467, 1475, 1491, 1497, 1515, 1522, 1530, 1540, 1545, 1553, 1559, 1564, 1575, 1577, 1581, 1586, 1590, 1601, 1602, 1610, 1618, 1621, 1638, 1658, 1664, 1670, 1674, 1675, 1679, 1683, 1692, 1694, 1699, 1702, 1707, 1722, 1723, 1724, 1731, 1739, 1740, 1744, 1748, 1750, 1753, 1758, 1760, 1768, 1776, 1802, 1805, 1807, 1808, 1814, 1816, 1818, 1833, 1855, 1866, 1868, 1871, 1912, 1918, 1929, 1932, 1936, 1951, 1966, 1967, 1971, 1972, 1977, 1979, 1984, 1991, 1994, 1996, 2005, 2006, 2017, 2019, 2023, 2032, 2033, 2042, 2045, 2051, 2062, 2066, 2077, 2085, 2086, 2087, 2092, 2095, 2098, 2106, 2110, 2112, 2114, 2145, 2147, 2148, 2152, 2182, 2187, 2192, 2202, 2207, 2213, 2219, 2229, 2258, 2264, 2276, 2277, 2280, 2285, 2288, 2299, 2304, 2305, 2308, 2316, 2340, 2343, 2345, 2361, 2367, 2370, 2377, 2385, 2390, 2407, 2422, 2435, 2443, 2445, 2454, 2470, 2478, 2483, 2492, 2502, 2505, 2508, 2518, 2523, 2524, 2528, 2529, 2533, 2547, 2574, 2578, 2583, 2584, 2602, 2605, 2610, 2613, 2615, 2637, 2640, 2641, 2650, 2651, 2653, 2654, 2666, 2668, 2671, 2682, 2686, 2696, 2722, 2733]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def is_effective_rubric(item):\n",
    "    \"\"\"Check if a rubric item is valid: include non-empty criterion, axis, and point fields\"\"\"\n",
    "    return (\n",
    "        isinstance(item, dict)\n",
    "        and bool(item.get(\"criterion\"))\n",
    "        and bool(item.get(\"axis\"))\n",
    "        and \"point\" in item\n",
    "    )\n",
    "\n",
    "empty_rubrics_final = []\n",
    "\n",
    "for i in tqdm(range(2735), desc=\"\ud83d\udd0d Checking for still-empty rubrics\"):\n",
    "    path = f\"outputs/rubrics/rubrics_{i}.json\"\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Directly determine whether there is no data or it is not a list\n",
    "        if not data or not isinstance(data, list):\n",
    "            empty_rubrics_final.append(i)\n",
    "            continue\n",
    "\n",
    "        # Determine whether all rubric items are invalid\n",
    "        if all(not is_effective_rubric(item) for item in data):\n",
    "            empty_rubrics_final.append(i)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Error reading {path}: {e}\")\n",
    "        empty_rubrics_final.append(i)\n",
    "\n",
    "print(f\"\\n\u274c Final empty rubrics count: {len(empty_rubrics_final)}\")\n",
    "print(f\"IDs: {empty_rubrics_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcc4 Example failure conversation:\n",
      "\n",
      "\n",
      "--- Conversation 1 --- (Length: 157 characters)\n",
      "User: What is recommended for a mild earache that won't go away after four days in Boston, Massachusetts? Should I go to urgent care or wait until next week?\n",
      "\n",
      "--- Conversation 10 --- (Length: 2674 characters)\n",
      "User: Help my check my moles\n",
      "Assistant: I'm sorry you're feeling concerned about your moles. Here are some steps you can take to keep track of them:\n",
      "\n",
      "### Self-check Routine:\n",
      "1. **Full-Body Exam**:\n",
      "   - Stand in a well-lit room with a full-length mirror.\n",
      "   - Use a handheld mirror or ask someone for help to see hard-to-reach areas like your back.\n",
      "\n",
      "2. **Know Your Baseline**:\n",
      "   - Familiarize yourself with existing spots, freckles, and moles so you can notice changes over time.\n",
      "\n",
      "3. **Document Any Changes**:\n",
      "   - Take pictures or notes if you notice a new mole or a change in shape, size, or color.\n",
      "\n",
      "Feel free to let me know if you'd like more specifics or have additional concerns!\n",
      "User: What if i find something that looks weird\n",
      "Assistant: If you see anything that catches your attention or seems unusual, here\u2019s what to watch for:\n",
      "\n",
      "### The ABCDE Rule:\n",
      "1. **A (Asymmetry)**: One half of the mole doesn\u2019t match the other.\n",
      "2. **B (Border)**: Irregular, scalloped, or poorly defined edges.\n",
      "3. **C (C\n",
      "\n",
      "--- Conversation 26 --- (Length: 2322 characters)\n",
      "User: Headache for the past few days\n",
      "Assistant: A dull headache that lasts for a few days can be triggered by a variety of factors, often related to everyday stressors or habits. Here's an overview of some possible causes:\n",
      "\n",
      "### 1. **Tension Headaches**\n",
      "   - **Description**: Typically feel like a tight band around the head, often triggered by stress, poor posture, or muscle tension.\n",
      "   - **Management**: Relaxation techniques, gentle stretching, warm compresses, and over-the-counter pain relievers can help.\n",
      "\n",
      "### 2. **Lifestyle Factors**\n",
      "   - **Sleep**: Irregular sleep patterns or insufficient rest.\n",
      "   - **Hydration**: Even mild dehydration can lead to headaches.\n",
      "   - **Caffeine**: Too much or too little can trigger headaches.\n",
      "\n",
      "### 3. **Environmental Triggers**\n",
      "   - **Screen Time**: Excessive use of digital devices can strain the eyes.\n",
      "   - **Lighting**: Bright or flickering lights.\n",
      "   - **Noise**: Constant loud sounds or chatter.\n",
      "\n",
      "### Notes\n",
      "   - If the headache intensifies, becomes more \n",
      "\n",
      "\ud83d\udcca Length statistics:\n",
      "\u2705 successful rubrics conversation mean length:716.60 \n",
      "\u274c failed rubrics conversation mean length:784.03 \n",
      "\ud83d\udcc8 The longest failed conversation:5350 \n",
      "\ud83d\udcc9 The shortest failed conversation:10 \n"
     ]
    }
   ],
   "source": [
    "# 2. print first few failed conversation\n",
    "print(\"\ud83d\udcc4 Example failure conversation:\\n\")\n",
    "for i in empty_rubrics_final[:3]:\n",
    "    conv_path = f\"outputs/prompts/conversation_{i}.txt\"\n",
    "    if os.path.exists(conv_path):\n",
    "        with open(conv_path) as f:\n",
    "            text = f.read()\n",
    "            print(f\"\\n--- Conversation {i} --- (Length: {len(text)} characters)\")\n",
    "            print(text[:1000])  # Print up to 1,000 characters \n",
    "\n",
    "# 3. Analyze the length distribution of failed conversations\n",
    "failed_lengths = []\n",
    "for i in empty_rubrics_final:\n",
    "    conv_path = f\"outputs/prompts/conversation_{i}.txt\"\n",
    "    if os.path.exists(conv_path):\n",
    "        with open(conv_path) as f:\n",
    "            text = f.read()\n",
    "            failed_lengths.append(len(text))\n",
    "\n",
    "# 4. Compare the successful conversation length\n",
    "successful_lengths = []\n",
    "for i in range(2735):\n",
    "    if i not in empty_rubrics_final:\n",
    "        conv_path = f\"outputs/prompts/conversation_{i}.txt\"\n",
    "        if os.path.exists(conv_path):\n",
    "            with open(conv_path) as f:\n",
    "                text = f.read()\n",
    "                successful_lengths.append(len(text))\n",
    "                \n",
    "# 5. Print statistical information\n",
    "print(\"\\n\ud83d\udcca Length statistics:\")\n",
    "print(f\"\u2705 successful rubrics conversation mean length:{sum(successful_lengths) / len(successful_lengths):.2f} \")\n",
    "print(f\"\u274c failed rubrics conversation mean length:{sum(failed_lengths) / len(failed_lengths):.2f} \")\n",
    "print(f\"\ud83d\udcc8 The longest failed conversation:{max(failed_lengths)} \")\n",
    "print(f\"\ud83d\udcc9 The shortest failed conversation:{min(failed_lengths)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in empty_rubrics_final:\n",
    "    try:\n",
    "        os.remove(f\"outputs/rubrics/rubrics_{i}.json\")\n",
    "    except:\n",
    "        pass\n",
    "print(f\"\u2705 Deleted {len(empty_rubrics_final)} empty rubrics files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJbeE0EBCDQ-"
   },
   "source": [
    "Rubrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ivjlHjU0Ba01"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n9_JD3s-HLof"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "054e8543722542a3895c380c8ca495e8",
      "e44792353ff84e2fa0760b09aca36591",
      "a158af212e574c4aa78864af26770677",
      "9c673f51bb874a52a2660d734740c503",
      "48bcd969e7124f71a5b4cc2cd23cb526",
      "8db59cea0c8448b9bf80b6c53b15e045",
      "686306f88c0346c787a395ef5acad5af",
      "f5bacc0b469449c18092f268329c6b9e",
      "8da37ce4d9794e9ca6aa9e845cd45591",
      "cf2e74d550374cba89b64bc21944a706",
      "df30601794d149dab8556fac7f140c30",
      "e63d7be235f54536b0594ca3441c8673",
      "46bcd2ca7dbe4f339fa8a5484015ce1b",
      "251594fb9862461e89739c7e58f099c7",
      "3783c03c4baa42e581dd047f198494a2",
      "1f291d2abace448091ca0e01a31dce22",
      "bfe838f0c5e945f1b6474b8ec66a1a82",
      "a9ac8e65fed148869429c0969378847b",
      "960e63ec423a4cdca97ef686396a34fa",
      "9cd22882150d4f6fb68ec8477a743ef3",
      "0dfbce436a9e4c03bf7e47a1e71e5e2c",
      "9573254d2bbd43389149a26f7c8c1d8e",
      "2e8939b8534a4ed09e2a98d46f86ec5d",
      "30e5ccbf30564a08b40e77c12b6e743d",
      "11bcf053977d4ea48a6dcd1ad7d48fa2",
      "3fa6294fd5d84c6fb5e369448d088515",
      "97e6f5876cb7403d91cc24dccbcf3478",
      "b968d93262ed4834842064522369b699",
      "826607e5dd404b288d3804bb13994b2a",
      "9d0acbbda4254a6d8cce93ee33e46a53",
      "077aac8d3b874c418703ea3d341ea97b",
      "140a8215f852424b87935b5c5967a567",
      "b21064eeccca449d8688dbf08a8a73bd",
      "22fa038f1aca4ecb96abf8a7c1789497",
      "2dd8f48589e14dee96c6aa9a7fc3d079",
      "8588015b838f431db6baafa66928ddb1",
      "b4f43bb19f444c8984b6167f5d71d43b",
      "3ff06e5f413e435e8323272fbade9b04",
      "c5924eccbb5b44e9ac8fee06529d5965",
      "0a4e9e95f2884a74bb10a93c33b72592",
      "920ec5655840499db9d08d519daac20f",
      "5e7ea97eafd647c1988120d7f42f026f",
      "6ecc7c6d4d1a4ee98c3bf335b867be3e",
      "175f376ea81e4e399d58f27295595c0d",
      "14f0f5631e424cd68024e3dd3362cde2",
      "02309290b400400da5a63c91021b2ec1",
      "9bad290c155d4610a822ae1e8728871d",
      "733eaf35e1b345fa92613e3c6b5792c2",
      "f663fb39684d48539f8a8acd6e091ee1",
      "3b871f2b0c8643c888e466e3fdcba120",
      "d5000e4e5cb848e1960017276c53a16f",
      "800c26750f234c7ea15a79a0ceb3db83",
      "78d7e1a138ee4fc795be55272a9ec42b",
      "f36097bb65b34ca6b4e989ad0da32909",
      "3a87b78423574c7695d3c7b86934a1c9",
      "db7cdccce293442580ddbd429cef1292",
      "4daa6c8882e14853a940d60d0ada9ab9",
      "c0be2785f0c24f74b0638cc0b0cb5880",
      "ead0b486bcf44df485abf275c28ca19d",
      "4b27a6c517184077ad1fb79df58f8673",
      "33240f126a9b4ae9b8b38b9f5861426d",
      "40c7769f33aa4f0085cff67df3110574",
      "2a87306fd76645fa8fd619e5286ef7fe",
      "adeb40f21ab34a96939d7313446fadc7",
      "29641e13a8f64ccda183326f5e33f119",
      "b3f61fb194014121a14846209c5145dc",
      "89c33ae9037c489fb6d85933ff31dcfd",
      "54d8caf1cf134f8da09e50b67f77280a",
      "d7076040634c4a8f93e57708fcc531ce",
      "c8d3cdd0318b474f801c648210b7afe3",
      "9d9d0ecabc474d62adb028a87f82fd93",
      "89ac6d20e9164ff29334f8164ddd878c",
      "9af59d3df18442388f7d1522b9b84b21",
      "4be437e556db46f8be0b7e54e0e21ae3",
      "6bade9ac165a4cc8bc4c6118b7c6edb1",
      "7935639699c2470189e90ba1ee7d13e1",
      "7050b1905ec04ef5adb9967925da6176",
      "1e71ba0b9d204d72b53169e225e8f9fb",
      "693e881773ed43f2a30e1a942d5e5ce1",
      "630cb0b1f1c2408b87a2992c27fdeedf",
      "ec536dd76e76443a9d85c142453fd919",
      "ee0de8ca57a742a8b7d171a58e560eec",
      "bdf303949b124284a35a738220efab38",
      "50ea4250960846298ded48cfd422d82a",
      "6beceb833136453585382f3b5b649b35",
      "0e40e9fe82c7403280502de56ba77e97",
      "0c82792faff24677a763cacb8facf9b2",
      "90fff9ca1e83437b8dfd83986b6dd70c",
      "e4812a673b574b7a9e2d941d98ce2e5a",
      "d9635bf4ff17470dba0285bc43c8a4c6",
      "7ead42b035a9440bb8e9f60c0869af9a",
      "56a29e2cc9014ae1ab72e26837854d94",
      "fe972ae330774196bd3b3f546b4c9947",
      "f2191521ad0b4df79768c743c31749ea",
      "065b32fb09914d12a7c6e1763d2746f8",
      "f8ea5334ceb64aa59e89b6dd5e56bf8e",
      "341a54dc65fd414e9da749db5b2dbf4f",
      "6c5a9deb3f0844ea87945752bc52d39a",
      "c1a83f8996ec4744995764b3950c6447",
      "4e9ed39563514e828f6c88e975b94c64",
      "b25446f586f94d98892a2fc766406557",
      "50f750ff85874c529e70c9bc2e849cae",
      "fa41165138f24ee4a31b6d25cca24261",
      "9372dfa6a9ea4186bfae15f56746d588",
      "ae517c3fdc8f4869a9b7b6ec1305212f",
      "d258632ba6014b27801e2f7ea083423e",
      "02c82ffb046946e683f5a96889435692",
      "815e0a34ae46413280a99effce31ad73",
      "504518fd553b4cc298da91264adf046b",
      "ccea687a44024a8fbd977133ec819bf7",
      "d3aca494e48f4701ae1a55fb6d5abc1e",
      "c2c06a78ce1149b1aeb998ce588de02d",
      "55a2368304a642e2954c94d47c1e3144",
      "132795a74a544e058682c402d84733f1",
      "15f044ce27784e47a7024db2c48cf36e",
      "aa701e51885a408abd0d08aca8a44f13",
      "bc295064737f48bcb0ebc576f8984ccf",
      "6ae3b3b54dac484ebbeeb48de1c1954f",
      "eb94b5224fb64c56bf7692f9e21bf7c7",
      "3a4280b9712043f888e93ddcfb546ea7",
      "53eb6cf8a7aa42b089a17d24d489b5b3"
     ]
    },
    "id": "61CmqhoQBawp",
    "outputId": "552465b0-890e-4f12-a836-ac36eb5c7916"
   },
   "outputs": [],
   "source": [
    "# \u2705 Load the embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "NG7i4AxIBap7"
   },
   "outputs": [],
   "source": [
    "# \u2705 Load reference rubrics\n",
    "def extract_ref_rubrics(dataset, output_dir=\"outputs/ref_rubrics\", max_items=None):\n",
    "    \"\"\"\n",
    "    Extract reference rubrics from filtered_dataset and store them as the ref_rubrics_{i}.json file by index.\n",
    "\n",
    "    Args:\n",
    "    - dataset: filtered HuggingFace dataset from which to extract referecne rubrics\n",
    "    - output_dir: save path\n",
    "    - max_items: Limit the number of saves (optional)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    saved = 0\n",
    "    for i, example in tqdm(enumerate(dataset), desc=\"Saving reference rubrics\"):\n",
    "        if max_items and saved >= max_items:\n",
    "            break\n",
    "\n",
    "        rubrics = example.get(\"rubrics\", None)\n",
    "        if rubrics:\n",
    "            out_path = os.path.join(output_dir, f\"ref_rubrics_{i}.json\")\n",
    "            with open(out_path, \"w\") as f:\n",
    "                json.dump(rubrics, f, indent=2, ensure_ascii=False)\n",
    "            saved += 1\n",
    "\n",
    "    print(f\"\u2705 Saved {saved} reference rubrics to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSO1Sbdp62-o",
    "outputId": "a4efd90c-8380-4d9f-a0b6-c8d2a3a2fcc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving reference rubrics: 2735it [00:00, 3469.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Saved 2735 reference rubrics to outputs/ref_rubrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_ref_rubrics(filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LIw18FWfEBU7"
   },
   "outputs": [],
   "source": [
    "# \u2705 All possible axes\n",
    "ALL_AXES = [\n",
    "    \"completeness\",\n",
    "    \"accuracy\",\n",
    "    \"context_awareness\",\n",
    "    \"communication_quality\",\n",
    "    \"instruction_following\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IOXps6BnO9Py"
   },
   "outputs": [],
   "source": [
    "def evaluate_rubrics(conversation_id, threshold=0.5, allow_axis_mismatch=True):\n",
    "    conversation_path = f\"outputs/prompts/conversation_{conversation_id}.txt\"\n",
    "    generated_path = f\"outputs/rubrics/rubrics_{conversation_id}.json\"\n",
    "    reference_path = f\"outputs/ref_rubrics/ref_rubrics_{conversation_id}.json\"\n",
    "    os.makedirs(\"outputs/eval\", exist_ok=True)\n",
    "    os.makedirs(\"outputs/eval/matched_pair\", exist_ok=True)\n",
    "    os.makedirs(\"outputs/eval/eval\", exist_ok=True)\n",
    "\n",
    "    with open(conversation_path) as f:\n",
    "        conversation = f.read().strip()\n",
    "\n",
    "    with open(reference_path) as f:\n",
    "        references = json.load(f)\n",
    "\n",
    "    with open(generated_path) as f:\n",
    "        generated = json.load(f)\n",
    "\n",
    "    ref_axes = set()\n",
    "    for r in references:\n",
    "        for tag in r.get(\"tags\", []):\n",
    "            if tag.startswith(\"axis:\"):\n",
    "                ref_axes.add(tag.split(\":\")[-1])\n",
    "    print(f\"\\n\ud83d\udcca Evaluation for Conversation #{conversation_id}\")\n",
    "    print(f\"\ud83d\udd0d Conversation Content:\\n{conversation[:300]}...\")\n",
    "    print(f\"Similarity Threshold: {threshold}\")\n",
    "    print(f\"Generated Rubrics: {len(generated)}\")\n",
    "    print(f\"Reference Rubrics: {len(references)}\")\n",
    "    print(f\"Axes in Reference: {sorted(ref_axes)}\")\n",
    "\n",
    "    match_results = []\n",
    "    axis_stats = {axis: {\"matched\": 0, \"score_sum\": 0.0, \"conversation_count\": 0} for axis in ALL_AXES}\n",
    "    axis_present = {axis: False for axis in ALL_AXES}\n",
    "\n",
    "    for ref in references:\n",
    "        ref_criterion = ref[\"criterion\"]\n",
    "        ref_point = ref.get(\"points\", 0)\n",
    "        ref_axis = next((tag.split(\":\")[-1] for tag in ref.get(\"tags\", []) if tag.startswith(\"axis:\")), None)\n",
    "\n",
    "        if ref_axis:\n",
    "            axis_present[ref_axis] = True\n",
    "\n",
    "        candidates = generated if allow_axis_mismatch else [g for g in generated if g.get(\"axis\") == ref_axis]\n",
    "        if not candidates:\n",
    "            continue\n",
    "\n",
    "        ref_emb = embedder.encode([ref_criterion])\n",
    "        gen_embs = embedder.encode([g[\"criterion\"] for g in candidates])\n",
    "        sims = cosine_similarity(ref_emb, gen_embs)[0]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        best_sim = float(sims[best_idx])\n",
    "        best_gen = candidates[best_idx]\n",
    "        gen_point = best_gen.get(\"point\", 0)\n",
    "        point_diff = abs(ref_point - gen_point)\n",
    "\n",
    "        # weight = 1.0 if point_diff <= 2 else 0.5 if point_diff <= 5 else 0.0\n",
    "        # \u2705 New weighting logic\n",
    "        if (ref_point >= 0 and gen_point >= 0) or (ref_point < 0 and gen_point < 0):\n",
    "            weight = 0.5  # If the positive or negative scores are the same, give the base score first\n",
    "        else:\n",
    "            weight = 0.0  # A score of 0 will be given directly if the positive or negative values are inconsistent\n",
    "\n",
    "        if point_diff <= 3:\n",
    "            weight += 0.5  # Precise consistency\n",
    "        elif point_diff <= 5:\n",
    "            weight += 0.3  # Roughly close\n",
    "        # If the score exceeds 5 points, no additional weight will be added\n",
    "\n",
    "        match_score = best_sim * weight\n",
    "\n",
    "        is_similar = best_sim >= threshold\n",
    "        is_axis_match = (best_gen.get(\"axis\") == ref_axis)\n",
    "\n",
    "        if ref_axis:\n",
    "            axis_stats[ref_axis][\"score_sum\"] += match_score\n",
    "            axis_stats[ref_axis][\"matched\"] += 1\n",
    "            axis_stats[ref_axis][\"avg_score\"] = (\n",
    "                axis_stats[ref_axis][\"score_sum\"] / axis_stats[ref_axis][\"matched\"]\n",
    "                if axis_stats[ref_axis][\"matched\"] > 0 else None\n",
    "            )\n",
    "\n",
    "        match_results.append({\n",
    "            \"ref_criterion\": ref_criterion,\n",
    "            \"ref_axis\": ref_axis,\n",
    "            \"ref_point\": ref_point,\n",
    "            \"gen_criterion\": best_gen[\"criterion\"],\n",
    "            \"gen_axis\": best_gen.get(\"axis\"),\n",
    "            \"gen_point\": gen_point,\n",
    "            \"similarity\": round(best_sim, 4),\n",
    "            \"point_diff\": point_diff,\n",
    "            \"score\": round(match_score, 3),\n",
    "            \"similar_match\": is_similar,\n",
    "            \"axis_match\": is_axis_match\n",
    "        })\n",
    "\n",
    "    for axis in ref_axes:\n",
    "        axis_stats[axis][\"conversation_count\"] += 1\n",
    "\n",
    "    matched_count = len(match_results)\n",
    "    total_ref = len(references)\n",
    "    avg_score = np.mean([r[\"score\"] for r in match_results]) if match_results else 0.0\n",
    "    avg_point_diff = np.mean([r[\"point_diff\"] for r in match_results]) if match_results else 0.0\n",
    "\n",
    "    print(f\"\\n\u2705 Matched: {matched_count} / {total_ref}\")\n",
    "    print(f\"Average Matching Score: {avg_score:.3f}\")\n",
    "    print(f\"Average Point Difference: {avg_point_diff:.2f}\")\n",
    "\n",
    "    print(\"\\n\ud83d\udcca Axis-level Breakdown:\")\n",
    "    for axis in ALL_AXES:\n",
    "        stat = axis_stats[axis]\n",
    "        if stat[\"conversation_count\"] == 0:\n",
    "            print(f\"- {axis:25s} | \u274c Missing in this conversation\")\n",
    "        else:\n",
    "            avg = stat[\"score_sum\"] / stat[\"matched\"] if stat[\"matched\"] > 0 else 0.0\n",
    "            print(f\"- {axis:25s} | Matched: {stat['matched']:2d} | Avg Score: {avg:.3f}\")\n",
    "\n",
    "    # Save\n",
    "    with open(f\"outputs/eval/matched_pair/matched_pairs_{conversation_id}.json\", \"w\") as f:\n",
    "        json.dump(match_results, f, indent=2)\n",
    "\n",
    "    summary = {\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"matched\": matched_count,\n",
    "        \"total_reference\": total_ref,\n",
    "        \"avg_score\": avg_score,\n",
    "        \"avg_point_diff\": avg_point_diff,\n",
    "        \"axis_stats\": axis_stats\n",
    "    }\n",
    "    with open(f\"outputs/eval/eval/eval_{conversation_id}.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_khRRHUcCPD-",
    "outputId": "67d27d11-e237-406b-d273-29582e66a783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcca Evaluation for Conversation #0\n",
      "\ud83d\udd0d Conversation Content:\n",
      "User: I\u2019m a 39 year old female. I\u2019m an emergency medicine physician. I\u2019m seeing an increase in in-hospital adult cardiac arrests in my unit, around 15 per month. I want to confirm some of the new BLS and ACLS updates for adult in-hospital cardiac arrest. Specifically, I\u2019d like the recommended compre...\n",
      "Similarity Threshold: 0.75\n",
      "Generated Rubrics: 10\n",
      "Reference Rubrics: 10\n",
      "Axes in Reference: ['accuracy', 'completeness']\n",
      "\n",
      "\u2705 Matched: 10 / 10\n",
      "Average Matching Score: 0.304\n",
      "Average Point Difference: 5.20\n",
      "\n",
      "\ud83d\udcca Axis-level Breakdown:\n",
      "- completeness              | Matched:  1 | Avg Score: 0.000\n",
      "- accuracy                  | Matched:  9 | Avg Score: 0.338\n",
      "- context_awareness         | \u274c Missing in this conversation\n",
      "- communication_quality     | \u274c Missing in this conversation\n",
      "- instruction_following     | \u274c Missing in this conversation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'conversation_id': 0,\n",
       " 'matched': 10,\n",
       " 'total_reference': 10,\n",
       " 'avg_score': np.float64(0.30379999999999996),\n",
       " 'avg_point_diff': np.float64(5.2),\n",
       " 'axis_stats': {'completeness': {'matched': 1,\n",
       "   'score_sum': 0.0,\n",
       "   'conversation_count': 1,\n",
       "   'avg_score': 0.0},\n",
       "  'accuracy': {'matched': 9,\n",
       "   'score_sum': 3.0381782054901123,\n",
       "   'conversation_count': 1,\n",
       "   'avg_score': 0.33757535616556805},\n",
       "  'context_awareness': {'matched': 0,\n",
       "   'score_sum': 0.0,\n",
       "   'conversation_count': 0},\n",
       "  'communication_quality': {'matched': 0,\n",
       "   'score_sum': 0.0,\n",
       "   'conversation_count': 0},\n",
       "  'instruction_following': {'matched': 0,\n",
       "   'score_sum': 0.0,\n",
       "   'conversation_count': 0}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "evaluate_rubrics(conversation_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_evaluate_rubrics(total_count, verbose=False):\n",
    "    os.makedirs(\"outputs/eval/summary\", exist_ok=True)\n",
    "    \n",
    "    all_scores = []\n",
    "    skipped = 0\n",
    "    \n",
    "    axis_global = {\n",
    "        axis: {\"score_sum\": 0.0, \"matched\": 0, \"count\": 0} for axis in ALL_AXES\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(total_count), desc=\"Evaluating rubrics\"):\n",
    "        gen_path = f\"outputs/rubrics/rubrics_{i}.json\"\n",
    "        ref_path = f\"outputs/ref_rubrics/ref_rubrics_{i}.json\"\n",
    "\n",
    "        # \ud83d\udca1 1. Skip if generated rubrics or reference rubrics don't exist\n",
    "        if not (os.path.exists(gen_path) and os.path.exists(ref_path)):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # \ud83d\udca1 2. Skip if generated rubrics is empty or only contains empty list / dict\n",
    "        with open(gen_path) as f:\n",
    "            gen_rubrics = json.load(f)\n",
    "        if not gen_rubrics or (isinstance(gen_rubrics, list) and len(gen_rubrics) == 0):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            summary = evaluate_rubrics(i)\n",
    "            all_scores.append(summary)\n",
    "            \n",
    "            axis_stats = summary.get(\"axis_stats\", {})\n",
    "            for axis in ALL_AXES:\n",
    "                axis_info = axis_stats.get(axis, {})\n",
    "                score_sum = axis_info.get(\"score_sum\", 0.0)\n",
    "                matched = axis_info.get(\"matched\", 0)\n",
    "                count = axis_info.get(\"conversation_count\", 0)\n",
    "\n",
    "                axis_global[axis][\"score_sum\"] += score_sum\n",
    "                axis_global[axis][\"matched\"] += matched\n",
    "                axis_global[axis][\"count\"] += count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Evaluation failed for conversation {i}: {e}\")\n",
    "            skipped += 1\n",
    "\n",
    "    # \u2705 Compute overall metrics\n",
    "    total_evals = len(all_scores)\n",
    "    if total_evals == 0:\n",
    "        print(\"\u26a0\ufe0f No successful evaluations.\")\n",
    "        return\n",
    "    \n",
    "    axis_global_avg = {\n",
    "        axis: round(axis_global[axis][\"score_sum\"] / axis_global[axis][\"matched\"], 4)\n",
    "        if axis_global[axis][\"matched\"] > 0 else None\n",
    "        for axis in ALL_AXES\n",
    "    }\n",
    "\n",
    "    avg_score = np.mean([s[\"avg_score\"] for s in all_scores])\n",
    "    avg_point_diff = np.mean([s[\"avg_point_diff\"] for s in all_scores])\n",
    "    total_matched = sum([s[\"matched\"] for s in all_scores])\n",
    "    total_ref = sum([s[\"total_reference\"] for s in all_scores])\n",
    "\n",
    "    print(f\"\\n\ud83c\udfaf Evaluation completed.\")\n",
    "    print(f\"\u2705 Successfully evaluated: {total_evals}\")\n",
    "    print(f\"\u26d4 Skipped: {skipped}\")\n",
    "    print(f\"\ud83d\udcca Overall avg match score: {avg_score:.3f}\")\n",
    "    print(f\"\ud83d\udcc9 Overall avg point difference: {avg_point_diff:.2f}\")\n",
    "    print(f\"\ud83d\udd22 Total matched / total reference: {total_matched} / {total_ref} ({total_matched/total_ref:.2%})\")\n",
    "\n",
    "    # save summary\n",
    "    with open(\"outputs/eval/summary/batch_eval_summary.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"evaluated\": total_evals,\n",
    "            \"skipped\": skipped,\n",
    "            \"overall_avg_score\": avg_score,\n",
    "            \"overall_avg_point_diff\": avg_point_diff,\n",
    "            \"global_axis_scores\": axis_global_avg,\n",
    "            \"per_axis_stats\": axis_global,\n",
    "            \"total_matched\": total_matched,\n",
    "            \"total_reference\": total_ref,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_evaluate_rubrics(total_count=2735)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24jqpO5t-zqI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fr_xycOT-zkr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (rubrics)",
   "language": "python",
   "name": "rubrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}